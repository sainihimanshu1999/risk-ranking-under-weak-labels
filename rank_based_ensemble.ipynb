{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b492a9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01802fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>if_score</th>\n",
       "      <th>lof_score</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.360532</td>\n",
       "      <td>0.977001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.326089</td>\n",
       "      <td>0.984578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333120</td>\n",
       "      <td>0.985754</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407142</td>\n",
       "      <td>1.029077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.327719</td>\n",
       "      <td>0.985974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   if_score  lof_score  isFraud\n",
       "0  0.360532   0.977001        0\n",
       "1  0.326089   0.984578        0\n",
       "2  0.333120   0.985754        0\n",
       "3  0.407142   1.029077        0\n",
       "4  0.327719   0.985974        0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load comparison table created earlier\n",
    "df = pd.read_csv(\"data/processed/if_lof_score_comparison.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "031b8f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>if_score</th>\n",
       "      <th>lof_score</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>if_rank</th>\n",
       "      <th>lof_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.360532</td>\n",
       "      <td>0.977001</td>\n",
       "      <td>0</td>\n",
       "      <td>28491.0</td>\n",
       "      <td>87745.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.326089</td>\n",
       "      <td>0.984578</td>\n",
       "      <td>0</td>\n",
       "      <td>68753.0</td>\n",
       "      <td>77026.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333120</td>\n",
       "      <td>0.985754</td>\n",
       "      <td>0</td>\n",
       "      <td>55218.0</td>\n",
       "      <td>75210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407142</td>\n",
       "      <td>1.029077</td>\n",
       "      <td>0</td>\n",
       "      <td>13562.0</td>\n",
       "      <td>21246.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.327719</td>\n",
       "      <td>0.985974</td>\n",
       "      <td>0</td>\n",
       "      <td>65095.0</td>\n",
       "      <td>74882.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   if_score  lof_score  isFraud  if_rank  lof_rank\n",
       "0  0.360532   0.977001        0  28491.0   87745.0\n",
       "1  0.326089   0.984578        0  68753.0   77026.0\n",
       "2  0.333120   0.985754        0  55218.0   75210.0\n",
       "3  0.407142   1.029077        0  13562.0   21246.0\n",
       "4  0.327719   0.985974        0  65095.0   74882.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"if_rank\"] = df[\"if_score\"].rank(method=\"first\", ascending=False)\n",
    "df[\"lof_rank\"] = df[\"lof_score\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bf557fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"avg_rank\"] = (df[\"if_rank\"] + df[\"lof_rank\"]) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d8789",
   "metadata": {},
   "source": [
    "Transactions ranked highly by either model move up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a501256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"min_rank\"] = df[[\"if_rank\", \"lof_rank\"]].min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa6b5f",
   "metadata": {},
   "source": [
    "If any model thinks it’s risky → treat as risky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d431040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"weighted_rank\"] = 0.6 * df[\"lof_rank\"] + 0.4 * df[\"if_rank\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e5349",
   "metadata": {},
   "source": [
    "From disagreement analysis:\n",
    "\t•\tLOF stronger at tight K\n",
    "\t•\tIF stronger deeper.So we weight LOF slightly higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df3eee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(ranks, labels, k_pct):\n",
    "    k = int(len(ranks) * k_pct / 100)\n",
    "    top_k = ranks.nsmallest(k).index\n",
    "    return labels.loc[top_k].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb8a483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision@0.1%\n",
      "IF only:        0.0\n",
      "LOF only:       0.21\n",
      "Avg rank:       0.13\n",
      "Min rank:       0.06\n",
      "Weighted rank:  0.23\n",
      "\n",
      "Precision@0.5%\n",
      "IF only:        0.148\n",
      "LOF only:       0.27\n",
      "Avg rank:       0.352\n",
      "Min rank:       0.164\n",
      "Weighted rank:  0.362\n",
      "\n",
      "Precision@1.0%\n",
      "IF only:        0.249\n",
      "LOF only:       0.235\n",
      "Avg rank:       0.329\n",
      "Min rank:       0.211\n",
      "Weighted rank:  0.32\n"
     ]
    }
   ],
   "source": [
    "for k in [0.1, 0.5, 1.0]:\n",
    "    print(f\"\\nPrecision@{k}%\")\n",
    "    print(\"IF only:       \", precision_at_k(df[\"if_rank\"], df[\"isFraud\"], k))\n",
    "    print(\"LOF only:      \", precision_at_k(df[\"lof_rank\"], df[\"isFraud\"], k))\n",
    "    print(\"Avg rank:      \", precision_at_k(df[\"avg_rank\"], df[\"isFraud\"], k))\n",
    "    print(\"Min rank:      \", precision_at_k(df[\"min_rank\"], df[\"isFraud\"], k))\n",
    "    print(\"Weighted rank: \", precision_at_k(df[\"weighted_rank\"], df[\"isFraud\"], k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7593b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraud captured @ 0.5%\n",
      "IF:        74\n",
      "LOF:       135\n",
      "Avg rank:  176\n",
      "Min rank:  82\n"
     ]
    }
   ],
   "source": [
    "def fraud_capture(ranks, labels, k_pct):\n",
    "    k = int(len(ranks) * k_pct / 100)\n",
    "    return labels.loc[ranks.nsmallest(k).index].sum()\n",
    "\n",
    "for k in [0.5]:\n",
    "    print(f\"\\nFraud captured @ {k}%\")\n",
    "    print(\"IF:       \", fraud_capture(df[\"if_rank\"], df[\"isFraud\"], k))\n",
    "    print(\"LOF:      \", fraud_capture(df[\"lof_rank\"], df[\"isFraud\"], k))\n",
    "    print(\"Avg rank: \", fraud_capture(df[\"avg_rank\"], df[\"isFraud\"], k))\n",
    "    print(\"Min rank: \", fraud_capture(df[\"min_rank\"], df[\"isFraud\"], k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76d958",
   "metadata": {},
   "source": [
    "### Rank-Based Ensemble Results\n",
    "\n",
    "Rank aggregation significantly improves fraud detection performance under fixed review capacity. At a 0.5% review threshold, the weighted rank ensemble achieves a precision of 36.2%, outperforming both Isolation Forest (14.8%) and LOF (27.0%), while capturing 176 fraud cases—substantially more than either individual model.\n",
    "\n",
    "These results demonstrate that combining global and local anomaly rankings via rank-based ensembling effectively exploits complementary risk signals without relying on label supervision or score calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a502b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Weighted rank (LOF weight = 0.5)\n",
      "  Precision@0.1%: 0.130\n",
      "  Precision@0.5%: 0.352\n",
      "  Precision@1.0%: 0.329\n",
      "\n",
      "Weighted rank (LOF weight = 0.6)\n",
      "  Precision@0.1%: 0.230\n",
      "  Precision@0.5%: 0.362\n",
      "  Precision@1.0%: 0.320\n",
      "\n",
      "Weighted rank (LOF weight = 0.7)\n",
      "  Precision@0.1%: 0.270\n",
      "  Precision@0.5%: 0.354\n",
      "  Precision@1.0%: 0.314\n"
     ]
    }
   ],
   "source": [
    "for w in [0.5, 0.6, 0.7]:\n",
    "    df[\"w_rank\"] = w * df[\"lof_rank\"] + (1 - w) * df[\"if_rank\"]\n",
    "    \n",
    "    print(f\"\\nWeighted rank (LOF weight = {w})\")\n",
    "    for k in [0.1, 0.5, 1.0]:\n",
    "        p = precision_at_k(df[\"w_rank\"], df[\"isFraud\"], k)\n",
    "        print(f\"  Precision@{k}%: {p:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5399d",
   "metadata": {},
   "source": [
    "### Weight Sensitivity Analysis\n",
    "\n",
    "Varying the relative contribution of LOF and Isolation Forest rankings results in smooth and predictable changes in Precision@K. Higher LOF weight improves performance under very tight review budgets, while balanced or IF-weighted rankings perform better at deeper review depths.\n",
    "\n",
    "Overall, ensemble performance remains stable across reasonable weight choices, indicating that gains are driven by complementary ranking signals rather than precise parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41c9b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\n",
    "    \"if_rank\",\n",
    "    \"lof_rank\",\n",
    "    \"avg_rank\",\n",
    "    \"min_rank\",\n",
    "    \"weighted_rank\",\n",
    "    \"isFraud\"\n",
    "]].to_csv(\"data/processed/rank_ensemble_scores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ff531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p1_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
